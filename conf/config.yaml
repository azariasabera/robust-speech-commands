# conf/config.yaml

defaults:
  - _self_

dataset:
  dataset_name: "speech_commands"
  root_dir: ${hydra:runtime.cwd}/data/speech
  download: false  # Set to true on first run to download the dataset
  num_classes: 12

audio:
  sr: 16000
  duration: 1 #sec

batch_size: 64

paths:
  train_path: ${hydra:runtime.cwd}/data/speech/numpy/train.npz
  val_path: ${hydra:runtime.cwd}/data/speech/numpy/val.npz
  test_path: ${hydra:runtime.cwd}/data/speech/numpy/test.npz

features:
  stft:
    n_fft: 1024
    win_length: 1024
    hop_length: 256
    window: hann
    center: true

  mel:
    n_fft: 1024
    win_length: 1024
    hop_length: 256
    n_mels: 64
    window: hann
    center: true
    fmin: 0.0
    fmax: 8000.0

  mfcc:
    n_fft: 1024
    win_length: 1024
    hop_length: 256
    n_mels: 64
    n_mfcc: 20
    window: hann
    center: true
    fmin: 0.0
    fmax: 8000.0
  
  settings:
    stft_to_db: true
    mel_to_db: true
    mfcc_include_deltas: true
    n_jobs: -1

feature_to_extract: 
  #- stft
  #- mel # e.g. python main.py feature_to_extract=['mel']
  - mfcc  

cnn:
  in_channels: 1
  base_channels: 16
  channel_multiplier: 2
  dropout_p: 0.1
  use_local_pool: true
  use_global_pool: true
  linear_hidden: 64
  kernel_size: [3,3]
  stride: [1,1]
  padding: [1,1]
  pool_size: [2,2]

training:
  epochs: 100
  batch_size: 64
  learning_rate: 1e-3
  weight_decay: 1e-4
  seed: 42
  deterministic: False

  device: auto   # auto | cpu | cuda

  save:
    enabled: true
    dir: ${hydra:runtime.cwd}/model/checkpoints
    save_best_only: true
    monitor: val_loss   # val_loss | val_acc
    filename: best_model.pt

  plotting:
    save: true
    dir: ${hydra:runtime.cwd}/model/plots
    filename: training_curves.png

  logging:
    print_every: 1

  early_stopping:
    enabled: true
    patience: 10

evaluation:
  plotting:
    save: true
    dir: ${training.plotting.dir}
    filename: confusion_matrix.png

noise:
  url: http://www.openslr.org/resources/17/musan.tar.gz
  dir: ${hydra:runtime.cwd}/data/noise
  shuffle: true
  sr: ${audio.sr}
  duration: ${audio.duration}
  seed: ${training.seed}
  snr: [0, 5, 10, 20] # in dB
  epsilon: 1e-8
  clip: true
  
  rms_threshold: 1e-4
  apply_threshold: true
  threshold_percentile: 20.0
  hangover_frames: 0

  # Min-statistics (blind PSD)
  min_stats_percentile: 10.0

  # Spectral subtraction tuning
  alpha: 1.0
  floor: 1e-10

  # In case when noise data is not given (e.g., real-time speech command capture)
  noise_psd_method: energy_vad # energy_vad or min_stats

realtime: 
  use_default: False # If a new trained model exists, should I use it or the default model
  sr: ${audio.sr}
  buffer_size: ${audio.duration} # we take 1 sec from the stream to check make final prediction  
  cooldown_time: 10 # in seconds
  hop_size: 0.1 # 100 ms (to extract mfcc and predict) e.g. if buffer is 1s and hop is 0.1s, then it means we make 10 predictions and take average/max of the predicted labels
  default_mean_path: ${hydra:runtime.cwd}/data/mean_and_std/mean.npz
  default_std_path: ${hydra:runtime.cwd}/data/mean_and_std/std.npz
  default_model_path_cpu: ${hydra:runtime.cwd}/model/checkpoints/best_model_cpu.pt
  default_model_path_gpu: ${hydra:runtime.cwd}/model/checkpoints/best_model_gpu.pt
  use_filter: False              # Whether to apply a noise reduction filter before feature extraction
  filter_type: weiner           # Type of filter to apply; options: 'weiner' | 'spectral_subtraction'
  selection: majority_vote      # Strategy to combine predictions from multiple hops; options: 'majority_vote' | 'max_average_confidence'